Foundational Flower Detector
(Implementation of the "Foundational Flower Detector" Architecture)
1. Proof of Concept & Scientific Contribution
This foundational phase is designed as a distinct, preliminary research step, preceding Phase 1 of the main research proposal. Its objective is to test and validate a core hypothesis: that a generic, pre-trained object detection model can be transformed into a high-precision scientific instrument capable of reliably identifying "flower" objects within chaotic, real-world imagery.
The scientific contribution, traceable to our architectural decision to "prioritize precision over recall," is the development of a methodology that systematically minimizes false positives. We will demonstrate that for complex, multi-stage scientific pipelines, the integrity of the entire analysis depends on the near-perfect purity of the initial data filter. Our asynchronous "Hard Negative Mining" cycle provides a novel and practical framework for achieving this, creating a model whose primary characteristic is not just accuracy, but trustworthiness. This work will establish a reproducible blueprint for creating reliable foundational detectors for specialized scientific domains.
2. Foundational Projects & Proper Citations
Our methodology is grounded in established, peer-reviewed work. The traceability of our project is ensured by building upon these foundational pillars:
â—	Mask R-CNN Architecture: The core model architecture is the Mask R-CNN, as proposed by He et al. (2017). This choice aligns with the ultimate goals of the main research proposal, which will require instance segmentation.
â—‹	He, K., Gkioxari, G., DollÃ¡r, P., & Girshick, R. (2017). Mask R-CNN. Proceedings of the IEEE International Conference on Computer Vision (ICCV).
â—	COCO Dataset: The model's foundational understanding of objects is derived from its pre-training on the Microsoft COCO dataset (Lin et al., 2014). This dataset will also serve as a primary source for our negative training examples, providing the "real-world" context necessary to challenge our model.
â—‹	Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., ... & Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. European Conference on Computer Vision (ECCV).





3. Data Sources
To fulfill the objective of this foundational phase, the data sources must be broad and general, teaching the model the core concept of a flower against a diverse background. They are distinct from the specialized datasets of the main research proposal.
â—	Positive Set (FLOWER): A large-scale, diverse collection of images containing real flowers.
â—‹	Source: The "Flower" class from the Open Images Dataset V7. This provides a large, pre-vetted set of images with existing bounding box annotations, covering a wide variety of species and conditions, which can accelerate our initial training. We will supplement this with targeted web scraping for specific morphological shapes if needed.
â—	Negative Set (BACKGROUND): A large-scale collection of images explicitly devoid of flowers.
â—‹	Source: The COCO Dataset. As previously planned, we will utilize the vast majority of COCO images that do not contain plant-related categories. This provides the necessary "mixed and random real-world objects" to train a robustly discriminating model.










4. Folder Structure
The project will be organized in a disciplined structure to ensure modularity and reproducibility.
foundational_detector/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ positive_images/  # Images from Open Images, etc.
â”‚   â”‚   â””â”€â”€ negative_images/  # Images from COCO, etc.
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ annotations/      # All COCO JSON annotation files
â”‚       â””â”€â”€ images/           # Symlinked images for training
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/          # To store model weights from each cycle
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â””â”€â”€ 02_results_analysis.ipynb
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preparation/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ inference/
â”‚   â””â”€â”€ verification_ui/      # UI for asynchronous human intervention
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

5. Data Structure: COCO JSON Annotations
All annotations for the FLOWER class will be managed in the standardized COCO JSON format. For this phase, we will utilize only the bounding box annotations to align with our specific task of object detection.
6. Pipeline Structure: The Asynchronous Human-in-the-Loop Cycle
This pipeline is the direct implementation of our agreed-upon architecture, designed for efficiency and traceability.
â—	Phase A: Initial Training
1.	Scripts in src/data_preparation/ prepare the initial dataset from the raw sources and generate train_annotations_v1.json.
2.	src/training/train.py trains the Mask R-CNN model on this data, saving the initial best model as model_v1.h5.
â—	Phase B: Automated Hard Negative Mining
1.	The src/training/find_hard_negatives.py script loads model_v1.h5 and runs inference on a large pool of unseen BACKGROUND images.
2.	All false positive detections are saved to a verification_queue.json file. This fulfills our architectural goal of systematically identifying the model's points of confusion.
â—	Phase C: Asynchronous Human Intervention
1.	The simple UI in src/verification_ui/ is run independently.
2.	It reads verification_queue.json and presents each false positive to the expert.
3.	The expert's confirmation ("This is NOT a flower") appends the image path to a confirmed_hard_negatives.log.
4.	This step is asynchronous. The main pipeline does not wait for it, allowing for a flexible and efficient workflow.
â—	Phase D: Reinforcement Training
1.	The data preparation script is re-run. It now reads confirmed_hard_negatives.log and incorporates these challenging examples into the training set.
2.	The train.py script is executed again, fine-tuning model_v1.h5 on the enriched dataset to produce a more precise model_v2.h5. The cycle then returns to Phase B.
7. Visualization of Training
We will use TensorBoard to log and visualize key metrics from each training run. As per our architectural decision to prioritize precision, the most critical visualization will be the Precision vs. Recall curve on the validation set, which we expect to show a steady increase in precision, potentially at the expense of recall, with each training cycle.
8. Key Packages and Libraries
â—	Core ML Framework: TensorFlow 2.x with the tf.keras API.
â—	Model Implementation: A well-maintained, TF2-compatible Mask R-CNN implementation.
â—	Data Handling: NumPy, Pandas, Pillow, OpenCV-Python.
â—	Annotation Handling: pycocotools.
â—	Visualization: TensorBoard, Matplotlib.
â—	Verification UI: Streamlit.










Diagram Flow
graph LR
    %% Subgraph for Data Ingestion and Dataset Building
    subgraph A [Data & Dataset Building]
        direction LR
        RD[ğŸ“ Raw Data<br/>Google Drive Images]:::dataNode --> BD[ğŸ”„ build_dataset.py<br/>Dataset Construction]:::processNode
        HNL_in[ğŸ“‹ Hard Negatives Log]:::dataNode --> BD
    end

    %% Subgraph for Model Training
    subgraph B [Model Training]
        direction LR
        AN[ğŸ“Š Annotations]:::dataNode --> TR[ğŸ§  train.py<br/>Model Training]:::processNode
    end

    %% Subgraph for Hard Negative Mining
    subgraph C [Hard Negative Mining]
        direction LR
        MD[ğŸ¯ Trained Model]:::modelNode --> FN[ğŸ” find_hard_negatives.py<br/>Failure Detection]:::processNode
        RD -- Background Images ---> FN
    end

    %% Subgraph for Human Review
    subgraph D [Human Review & Feedback Loop]
        direction LR
        VQ[ğŸ“¤ Verification Queue]:::dataNode --> HR[ğŸ‘¤ app.py Streamlit<br/>Human Validation]:::humanNode
    end

    %% Connections between Subgraphs and main nodes
    BD -- Generates --> AN
    TR -- Produces --> MD
    MD -.->|Previous Model| TR
    FN -- Creates --> VQ
    HR -- Confirms Negatives --> HNL_out[ğŸ“‹ Hard Negatives Log<br/>(Updated)]:::dataNode

    %% Styling (Kept original styling as requested)
    classDef dataNode fill:#3b82f6,stroke:#1d4ed8,stroke-width:2px,color:#fff
    classDef processNode fill:#10b981,stroke:#059669,stroke-width:2px,color:#fff
    classDef humanNode fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
    classDef modelNode fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff

    class RD,HNL_in,AN,VQ,HNL_out dataNode
    class BD,TR,FN processNode
    class HR humanNode
    class MD modelNode


